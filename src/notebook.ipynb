{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.visualization.plot_images_grid import plot_images_grid\n",
    "from deepSVDD import DeepSVDD\n",
    "from datasets.main import load_dataset\n"
   ]
  },
  {
   "source": [
    "## Preparing config & datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Default Setting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_config = None\n",
    "nu = 0.01\n",
    "seed = -1\n",
    "n_jobs_dataloader = 0\n",
    "load_model = None\n",
    "optimizer_name = 'adam'\n",
    "ae_optimizer_name = 'adam'\n",
    "device = 'cuda'\n",
    "apply_model = True\n"
   ]
  },
  {
   "source": [
    "### CIFAR-10"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'cifar10'\n",
    "# net_name = 'cifar10_LeNet'\n",
    "# xp_path = '../log/cifar10_test'\n",
    "# data_path = '../data'\n",
    "# objective = 'one-class'\n",
    "# lr = 0.0001\n",
    "# n_epochs = 150\n",
    "# lr_milestone = [50]\n",
    "# batch_size = 200\n",
    "# weight_decay = 0.5e-6\n",
    "# pretrain = True\n",
    "# ae_lr = 0.0001\n",
    "# ae_n_epochs = 350\n",
    "# ae_lr_milestone = [250]\n",
    "# ae_batch_size = 200\n",
    "# ae_weight_decay = 0.5e-6\n",
    "# normal_class = 3"
   ]
  },
  {
   "source": [
    "### MNIST"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'mnist'\n",
    "net_name = 'mnist_LeNet'\n",
    "xp_path = '../log/mnist_test'\n",
    "data_path = '../data'\n",
    "objective = 'one-class'\n",
    "lr = 0.0001\n",
    "n_epochs = 150\n",
    "lr_milestone = [50]\n",
    "batch_size = 200\n",
    "weight_decay = 0.5e-6\n",
    "pretrain = True\n",
    "ae_lr = 0.0001\n",
    "ae_n_epochs = 150\n",
    "ae_lr_milestone = [50]\n",
    "ae_batch_size = 200\n",
    "ae_weight_decay = 0.5e-3\n",
    "normal_class = 3"
   ]
  },
  {
   "source": [
    "## Load Config"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get configuration\n",
    "cfg = Config(locals().copy())\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "log_file = xp_path + '/log.txt'\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n"
   ]
  },
  {
   "source": [
    "## Add Logs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Log file is ../log/mnist_test/log.txt.\n",
      "INFO:root:Data path is ../data.\n",
      "INFO:root:Export path is ../log/mnist_test.\n",
      "INFO:root:Dataset: mnist\n",
      "INFO:root:Normal class: 3\n",
      "INFO:root:Network: mnist_LeNet\n",
      "INFO:root:Deep SVDD objective: one-class\n",
      "INFO:root:Nu-paramerter: 0.01\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of dataloader workers: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print arguments\n",
    "logger.info('Log file is %s.' % log_file)\n",
    "logger.info('Data path is %s.' % data_path)\n",
    "logger.info('Export path is %s.' % xp_path)\n",
    "\n",
    "logger.info('Dataset: %s' % dataset_name)\n",
    "logger.info('Normal class: %d' % normal_class)\n",
    "logger.info('Network: %s' % net_name)\n",
    "\n",
    "# If specified, load experiment config from JSON-file\n",
    "if load_config:\n",
    "    cfg.load_config(import_json=load_config)\n",
    "    logger.info('Loaded configuration from %s.' % load_config)\n",
    "\n",
    "# Print configuration\n",
    "logger.info('Deep SVDD objective: %s' % cfg.settings['objective'])\n",
    "logger.info('Nu-paramerter: %.2f' % cfg.settings['nu'])\n",
    "\n",
    "# Set seed\n",
    "if cfg.settings['seed'] != -1:\n",
    "    random.seed(cfg.settings['seed'])\n",
    "    np.random.seed(cfg.settings['seed'])\n",
    "    torch.manual_seed(cfg.settings['seed'])\n",
    "    logger.info('Set seed to %d.' % cfg.settings['seed'])\n",
    "\n",
    "# Default device to 'cpu' if cuda is not available\n",
    "if not torch.cuda.is_available():\n",
    "    device = 'cpu'\n",
    "logger.info('Computation device: %s' % device)\n",
    "logger.info('Number of dataloader workers: %d' % n_jobs_dataloader)"
   ]
  },
  {
   "source": [
    "## Load Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = load_dataset(dataset_name, data_path, normal_class)"
   ]
  },
  {
   "source": [
    "## Setup model and network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepSVDD model and set neural network \\phi\n",
    "deep_SVDD = DeepSVDD(cfg.settings['objective'], cfg.settings['nu'])\n",
    "deep_SVDD.set_network(net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Pretraining: True\n"
     ]
    }
   ],
   "source": [
    "# If specified, load Deep SVDD model (radius R, center c, network weights, and possibly autoencoder weights)\n",
    "if load_model:\n",
    "    deep_SVDD.load_model(model_path=load_model, load_ae=True)\n",
    "    logger.info('Loading model from %s.' % load_model)\n",
    "\n",
    "logger.info('Pretraining: %s' % pretrain)"
   ]
  },
  {
   "source": [
    "### Using Pre-Trained"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 150\n",
      "INFO:root:Pretraining learning rate scheduler milestones: [50]\n",
      "INFO:root:Pretraining batch size: 200\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:  Epoch 1/150\t Time: 8.529\t Loss: 153.09089734\n",
      "INFO:root:  Epoch 2/150\t Time: 7.836\t Loss: 104.41770566\n",
      "INFO:root:  Epoch 3/150\t Time: 7.558\t Loss: 75.85939297\n",
      "INFO:root:  Epoch 4/150\t Time: 7.579\t Loss: 57.18945239\n",
      "INFO:root:  Epoch 5/150\t Time: 7.907\t Loss: 44.01152518\n",
      "INFO:root:  Epoch 6/150\t Time: 7.028\t Loss: 34.53868841\n",
      "INFO:root:  Epoch 7/150\t Time: 6.223\t Loss: 27.81984766\n",
      "INFO:root:  Epoch 8/150\t Time: 6.637\t Loss: 23.02433888\n",
      "INFO:root:  Epoch 9/150\t Time: 6.410\t Loss: 19.59991713\n",
      "INFO:root:  Epoch 10/150\t Time: 6.422\t Loss: 17.05780146\n",
      "INFO:root:  Epoch 11/150\t Time: 6.330\t Loss: 15.09773337\n",
      "INFO:root:  Epoch 12/150\t Time: 6.539\t Loss: 13.54791915\n",
      "INFO:root:  Epoch 13/150\t Time: 6.045\t Loss: 12.31806091\n",
      "INFO:root:  Epoch 14/150\t Time: 6.533\t Loss: 11.31291233\n",
      "INFO:root:  Epoch 15/150\t Time: 6.405\t Loss: 10.48695020\n",
      "INFO:root:  Epoch 16/150\t Time: 6.705\t Loss: 9.78166608\n",
      "INFO:root:  Epoch 17/150\t Time: 6.332\t Loss: 9.18102747\n",
      "INFO:root:  Epoch 18/150\t Time: 6.277\t Loss: 8.68017234\n",
      "INFO:root:  Epoch 19/150\t Time: 6.591\t Loss: 8.25566544\n",
      "INFO:root:  Epoch 20/150\t Time: 7.062\t Loss: 7.89078194\n",
      "INFO:root:  Epoch 21/150\t Time: 6.581\t Loss: 7.57184387\n",
      "INFO:root:  Epoch 22/150\t Time: 6.307\t Loss: 7.28446673\n",
      "INFO:root:  Epoch 23/150\t Time: 6.498\t Loss: 7.02098886\n",
      "INFO:root:  Epoch 24/150\t Time: 6.351\t Loss: 6.79448278\n",
      "INFO:root:  Epoch 25/150\t Time: 6.328\t Loss: 6.59169426\n",
      "INFO:root:  Epoch 26/150\t Time: 6.600\t Loss: 6.41271365\n",
      "INFO:root:  Epoch 27/150\t Time: 6.627\t Loss: 6.24663482\n",
      "INFO:root:  Epoch 28/150\t Time: 6.478\t Loss: 6.09733040\n",
      "INFO:root:  Epoch 29/150\t Time: 6.412\t Loss: 5.96125420\n",
      "INFO:root:  Epoch 30/150\t Time: 6.362\t Loss: 5.83415284\n",
      "INFO:root:  Epoch 31/150\t Time: 6.671\t Loss: 5.71848725\n",
      "INFO:root:  Epoch 32/150\t Time: 6.550\t Loss: 5.61035175\n",
      "INFO:root:  Epoch 33/150\t Time: 6.383\t Loss: 5.51319756\n",
      "INFO:root:  Epoch 34/150\t Time: 6.480\t Loss: 5.41823212\n",
      "INFO:root:  Epoch 35/150\t Time: 6.595\t Loss: 5.33224275\n",
      "INFO:root:  Epoch 36/150\t Time: 6.490\t Loss: 5.24844391\n",
      "INFO:root:  Epoch 37/150\t Time: 6.274\t Loss: 5.16953581\n",
      "INFO:root:  Epoch 38/150\t Time: 6.385\t Loss: 5.10418224\n",
      "INFO:root:  Epoch 39/150\t Time: 6.193\t Loss: 5.03423242\n",
      "INFO:root:  Epoch 40/150\t Time: 5.986\t Loss: 4.97252924\n",
      "INFO:root:  Epoch 41/150\t Time: 5.990\t Loss: 4.91230034\n",
      "INFO:root:  Epoch 42/150\t Time: 6.210\t Loss: 4.85554724\n",
      "INFO:root:  Epoch 43/150\t Time: 6.122\t Loss: 4.80049918\n",
      "INFO:root:  Epoch 44/150\t Time: 6.017\t Loss: 4.74585173\n",
      "INFO:root:  Epoch 45/150\t Time: 5.952\t Loss: 4.69719728\n",
      "INFO:root:  Epoch 46/150\t Time: 6.047\t Loss: 4.64629206\n",
      "INFO:root:  Epoch 47/150\t Time: 6.027\t Loss: 4.60221111\n",
      "INFO:root:  Epoch 48/150\t Time: 5.941\t Loss: 4.55772766\n",
      "INFO:root:  Epoch 49/150\t Time: 5.961\t Loss: 4.51559467\n",
      "INFO:root:  Epoch 50/150\t Time: 6.159\t Loss: 4.48934760\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:  Epoch 51/150\t Time: 6.250\t Loss: 4.48844467\n",
      "INFO:root:  Epoch 52/150\t Time: 6.005\t Loss: 4.47980516\n",
      "INFO:root:  Epoch 53/150\t Time: 5.996\t Loss: 4.47527793\n",
      "INFO:root:  Epoch 54/150\t Time: 6.102\t Loss: 4.47289767\n",
      "INFO:root:  Epoch 55/150\t Time: 6.221\t Loss: 4.46727471\n",
      "INFO:root:  Epoch 56/150\t Time: 6.466\t Loss: 4.46217612\n",
      "INFO:root:  Epoch 57/150\t Time: 6.464\t Loss: 4.46094428\n",
      "INFO:root:  Epoch 58/150\t Time: 6.597\t Loss: 4.45784381\n",
      "INFO:root:  Epoch 59/150\t Time: 6.467\t Loss: 4.45166842\n",
      "INFO:root:  Epoch 60/150\t Time: 6.499\t Loss: 4.44634959\n",
      "INFO:root:  Epoch 61/150\t Time: 6.372\t Loss: 4.43989537\n",
      "INFO:root:  Epoch 62/150\t Time: 6.492\t Loss: 4.43618547\n",
      "INFO:root:  Epoch 63/150\t Time: 6.365\t Loss: 4.43320385\n",
      "INFO:root:  Epoch 64/150\t Time: 6.293\t Loss: 4.42566303\n",
      "INFO:root:  Epoch 65/150\t Time: 6.409\t Loss: 4.42205726\n",
      "INFO:root:  Epoch 66/150\t Time: 6.652\t Loss: 4.41406095\n",
      "INFO:root:  Epoch 67/150\t Time: 6.571\t Loss: 4.41003547\n",
      "INFO:root:  Epoch 68/150\t Time: 6.441\t Loss: 4.40676505\n",
      "INFO:root:  Epoch 69/150\t Time: 6.534\t Loss: 4.40209581\n",
      "INFO:root:  Epoch 70/150\t Time: 6.448\t Loss: 4.39457569\n",
      "INFO:root:  Epoch 71/150\t Time: 6.423\t Loss: 4.39223363\n",
      "INFO:root:  Epoch 72/150\t Time: 6.686\t Loss: 4.38384467\n",
      "INFO:root:  Epoch 73/150\t Time: 6.707\t Loss: 4.38288318\n",
      "INFO:root:  Epoch 74/150\t Time: 6.754\t Loss: 4.37437688\n",
      "INFO:root:  Epoch 75/150\t Time: 6.684\t Loss: 4.36807496\n",
      "INFO:root:  Epoch 76/150\t Time: 6.801\t Loss: 4.36321002\n",
      "INFO:root:  Epoch 77/150\t Time: 6.881\t Loss: 4.35638921\n",
      "INFO:root:  Epoch 78/150\t Time: 6.775\t Loss: 4.35366097\n",
      "INFO:root:  Epoch 79/150\t Time: 6.753\t Loss: 4.34877945\n",
      "INFO:root:  Epoch 80/150\t Time: 6.969\t Loss: 4.33945971\n",
      "INFO:root:  Epoch 81/150\t Time: 6.886\t Loss: 4.33225709\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-c7791d668c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                         \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ae_weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                         \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                         n_jobs_dataloader=n_jobs_dataloader)\n\u001b[0m",
      "\u001b[0;32m~/Projects/Deep-SVDD-PyTorch/src/deepSVDD.py\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(self, dataset, optimizer_name, lr, n_epochs, lr_milestones, batch_size, weight_decay, device, n_jobs_dataloader)\u001b[0m\n\u001b[1;32m     96\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                                     n_jobs_dataloader=n_jobs_dataloader)\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mae_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_network_weights_from_pretraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Deep-SVDD-PyTorch/src/optim/ae_trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset, ae_net)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info('Pretraining: %s' % pretrain)\n",
    "if pretrain:\n",
    "    # Log pretraining details\n",
    "    logger.info('Pretraining optimizer: %s' % cfg.settings['ae_optimizer_name'])\n",
    "    logger.info('Pretraining learning rate: %g' % cfg.settings['ae_lr'])\n",
    "    logger.info('Pretraining epochs: %d' % cfg.settings['ae_n_epochs'])\n",
    "    logger.info('Pretraining learning rate scheduler milestones: %s' % (cfg.settings['ae_lr_milestone'],))\n",
    "    logger.info('Pretraining batch size: %d' % cfg.settings['ae_batch_size'])\n",
    "    logger.info('Pretraining weight decay: %g' % cfg.settings['ae_weight_decay'])\n",
    "\n",
    "    # Pretrain model on dataset (via autoencoder)\n",
    "    deep_SVDD.pretrain(dataset,\n",
    "                        optimizer_name=cfg.settings['ae_optimizer_name'],\n",
    "                        lr=cfg.settings['ae_lr'],\n",
    "                        n_epochs=cfg.settings['ae_n_epochs'],\n",
    "                        lr_milestones=cfg.settings['ae_lr_milestone'],\n",
    "                        batch_size=cfg.settings['ae_batch_size'],\n",
    "                        weight_decay=cfg.settings['ae_weight_decay'],\n",
    "                        device=device,\n",
    "                        n_jobs_dataloader=n_jobs_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Log training details\n",
    "logger.info('Training optimizer: %s' % cfg.settings['optimizer_name'])\n",
    "logger.info('Training learning rate: %g' % cfg.settings['lr'])\n",
    "logger.info('Training epochs: %d' % cfg.settings['n_epochs'])\n",
    "logger.info('Training learning rate scheduler milestones: %s' % (cfg.settings['lr_milestone'],))\n",
    "logger.info('Training batch size: %d' % cfg.settings['batch_size'])\n",
    "logger.info('Training weight decay: %g' % cfg.settings['weight_decay'])\n"
   ]
  },
  {
   "source": [
    "## Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on dataset\n",
    "deep_SVDD.train(dataset,\n",
    "                optimizer_name=cfg.settings['optimizer_name'],\n",
    "                lr=cfg.settings['lr'],\n",
    "                n_epochs=cfg.settings['n_epochs'],\n",
    "                lr_milestones=cfg.settings['lr_milestone'],\n",
    "                batch_size=cfg.settings['batch_size'],\n",
    "                weight_decay=cfg.settings['weight_decay'],\n",
    "                device=device,\n",
    "                n_jobs_dataloader=n_jobs_dataloader)"
   ]
  },
  {
   "source": [
    "## Evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_SVDD.test(dataset, device=device, n_jobs_dataloader=n_jobs_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot most anomalous and most normal (within-class) test samples\n",
    "indices, labels, scores = zip(*deep_SVDD.results['test_scores'])\n",
    "indices, labels, scores = np.array(indices), np.array(labels), np.array(scores)\n",
    "idx_sorted = indices[labels == 0][np.argsort(scores[labels == 0])]  # sorted from lowest to highest anomaly score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name in ('mnist', 'cifar10', 'custom'):\n",
    "\n",
    "    if dataset_name == 'mnist':\n",
    "        X_normals = dataset.test_set.test_data[idx_sorted[:32], ...].unsqueeze(1)\n",
    "        X_outliers = dataset.test_set.test_data[idx_sorted[-32:], ...].unsqueeze(1)\n",
    "\n",
    "    if dataset_name == 'cifar10':\n",
    "        X_normals = torch.tensor(np.transpose(dataset.test_set.test_data[idx_sorted[:32], ...], (0, 3, 1, 2)))\n",
    "        X_outliers = torch.tensor(np.transpose(dataset.test_set.test_data[idx_sorted[-32:], ...], (0, 3, 1, 2)))\n",
    "\n",
    "    if dataset_name == 'custom':\n",
    "        X_normals = torch.tensor(dataset.test_set[idx_sorted[:32], ...])\n",
    "        X_outliers = torch.tensor(dataset.test_set[idx_sorted[-32:], ...])\n",
    "\n",
    "    plot_images_grid(X_normals, export_img=xp_path + '/normals', title='Most normal examples', padding=2)\n",
    "    plot_images_grid(X_outliers, export_img=xp_path + '/outliers', title='Most anomalous examples', padding=2)\n"
   ]
  },
  {
   "source": [
    "## Save model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results, model, and configuration\n",
    "deep_SVDD.save_results(export_json=xp_path + '/results.json')\n",
    "deep_SVDD.save_model(export_model=xp_path + '/model.tar')\n",
    "cfg.save_config(export_json=xp_path + '/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model and print to file\n",
    "if apply_model:\n",
    "    deep_SVDD.apply_model(export_file=xp_path + '/apply_output.txt', dataset=dataset, device=device, n_jobs_dataloader=n_jobs_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}