{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from utils.config import Config\n",
    "from utils.visualization.plot_images_grid import plot_images_grid, show_image\n",
    "from deepSVDD import DeepSVDD\n",
    "from datasets.main import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing config & datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_config = None\n",
    "nu = 0.01\n",
    "seed = -1\n",
    "n_jobs_dataloader = 0\n",
    "load_model = None\n",
    "optimizer_name = 'adam'\n",
    "ae_optimizer_name = 'adam'\n",
    "device = 'cuda'\n",
    "apply_model = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'cifar10'\n",
    "# net_name = 'cifar10_LeNet'\n",
    "# xp_path = '../log/cifar10_test'\n",
    "# data_path = '../data'\n",
    "# objective = 'one-class'\n",
    "# lr = 0.0001\n",
    "# n_epochs = 150\n",
    "# lr_milestone = [50]\n",
    "# batch_size = 200\n",
    "# weight_decay = 0.5e-6\n",
    "# pretrain = True\n",
    "# ae_lr = 0.0001\n",
    "# ae_n_epochs = 350\n",
    "# ae_lr_milestone = [250]\n",
    "# ae_batch_size = 200\n",
    "# ae_weight_decay = 0.5e-6\n",
    "# normal_class = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'mnist'\n",
    "# net_name = 'mnist_LeNet'\n",
    "# xp_path = '../log/mnist_test'\n",
    "# data_path = '../data'\n",
    "# objective = 'one-class'\n",
    "# lr = 0.0001\n",
    "# n_epochs = 150\n",
    "# lr_milestone = [50]\n",
    "# batch_size = 200\n",
    "# weight_decay = 0.5e-6\n",
    "# pretrain = True\n",
    "# ae_lr = 0.0001\n",
    "# ae_n_epochs = 150\n",
    "# ae_lr_milestone = [50]\n",
    "# ae_batch_size = 200\n",
    "# ae_weight_decay = 0.5e-3\n",
    "# normal_class = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'custom'\n",
    "net_name = 'mnist_LeNet'\n",
    "xp_path = '../log/custom'\n",
    "data_path = '../data'\n",
    "objective = 'one-class'\n",
    "lr = 0.0001\n",
    "n_epochs = 150\n",
    "lr_milestone = [50]\n",
    "batch_size = 200\n",
    "weight_decay = 0.5e-6\n",
    "pretrain = True\n",
    "ae_lr = 0.0001\n",
    "ae_n_epochs = 150\n",
    "ae_lr_milestone = [50]\n",
    "ae_batch_size = 200\n",
    "ae_weight_decay = 0.5e-3\n",
    "normal_class = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get configuration\n",
    "cfg = Config(locals().copy())\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "log_file = xp_path + '/log.txt'\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Log file is ../log/custom/log.txt.\n",
      "INFO:root:Data path is ../data.\n",
      "INFO:root:Export path is ../log/custom.\n",
      "INFO:root:Dataset: custom\n",
      "INFO:root:Normal class: 3\n",
      "INFO:root:Network: mnist_LeNet\n",
      "INFO:root:Deep SVDD objective: one-class\n",
      "INFO:root:Nu-paramerter: 0.01\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of dataloader workers: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print arguments\n",
    "logger.info('Log file is %s.' % log_file)\n",
    "logger.info('Data path is %s.' % data_path)\n",
    "logger.info('Export path is %s.' % xp_path)\n",
    "\n",
    "logger.info('Dataset: %s' % dataset_name)\n",
    "logger.info('Normal class: %d' % normal_class)\n",
    "logger.info('Network: %s' % net_name)\n",
    "\n",
    "# If specified, load experiment config from JSON-file\n",
    "if load_config:\n",
    "    cfg.load_config(import_json=load_config)\n",
    "    logger.info('Loaded configuration from %s.' % load_config)\n",
    "\n",
    "# Print configuration\n",
    "logger.info('Deep SVDD objective: %s' % cfg.settings['objective'])\n",
    "logger.info('Nu-paramerter: %.2f' % cfg.settings['nu'])\n",
    "\n",
    "# Set seed\n",
    "if cfg.settings['seed'] != -1:\n",
    "    random.seed(cfg.settings['seed'])\n",
    "    np.random.seed(cfg.settings['seed'])\n",
    "    torch.manual_seed(cfg.settings['seed'])\n",
    "    logger.info('Set seed to %d.' % cfg.settings['seed'])\n",
    "\n",
    "# Default device to 'cpu' if cuda is not available\n",
    "if not torch.cuda.is_available():\n",
    "    device = 'cpu'\n",
    "logger.info('Computation device: %s' % device)\n",
    "logger.info('Number of dataloader workers: %d' % n_jobs_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset = load_dataset(dataset_name, data_path, normal_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_set.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7fbce8190ed0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72, 73, 74, 75])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack(dataset.train_set.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbce8f73210>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "idx_no = 70\n",
    "transform = transforms.ToPILImage()\n",
    "plt.title(dataset.train_set.dataset[idx_no][1])\n",
    "plt.imshow(transform(dataset.train_set.dataset[idx_no][0]), cmap='gray', vmin=0, vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup model and network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DeepSVDD model and set neural network \\phi\n",
    "deep_SVDD = DeepSVDD(cfg.settings['objective'], cfg.settings['nu'])\n",
    "deep_SVDD.set_network(net_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pretraining: True\n"
     ]
    }
   ],
   "source": [
    "# If specified, load Deep SVDD model (radius R, center c, network weights, and possibly autoencoder weights)\n",
    "if load_model:\n",
    "    deep_SVDD.load_model(model_path=load_model, load_ae=True)\n",
    "    logger.info('Loading model from %s.' % load_model)\n",
    "\n",
    "logger.info('Pretraining: %s' % pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional] Using Pre-Trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 150\n",
      "INFO:root:Pretraining learning rate scheduler milestones: [50]\n",
      "INFO:root:Pretraining batch size: 200\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "INFO:root:  Epoch 1/150\t Time: 0.098\t Loss: 134.38822937\n",
      "INFO:root:  Epoch 2/150\t Time: 0.082\t Loss: 131.99121094\n",
      "INFO:root:  Epoch 3/150\t Time: 0.078\t Loss: 129.68475342\n",
      "INFO:root:  Epoch 4/150\t Time: 0.072\t Loss: 127.43898773\n",
      "INFO:root:  Epoch 5/150\t Time: 0.069\t Loss: 125.23738098\n",
      "INFO:root:  Epoch 6/150\t Time: 0.081\t Loss: 123.07717133\n",
      "INFO:root:  Epoch 7/150\t Time: 0.085\t Loss: 120.96813202\n",
      "INFO:root:  Epoch 8/150\t Time: 0.073\t Loss: 118.93139648\n",
      "INFO:root:  Epoch 9/150\t Time: 0.075\t Loss: 116.94622803\n",
      "INFO:root:  Epoch 10/150\t Time: 0.071\t Loss: 114.99033356\n",
      "INFO:root:  Epoch 11/150\t Time: 0.084\t Loss: 113.08424377\n",
      "INFO:root:  Epoch 12/150\t Time: 0.089\t Loss: 111.23997498\n",
      "INFO:root:  Epoch 13/150\t Time: 0.088\t Loss: 109.45610809\n",
      "INFO:root:  Epoch 14/150\t Time: 0.083\t Loss: 107.72515869\n",
      "INFO:root:  Epoch 15/150\t Time: 0.088\t Loss: 106.05533600\n",
      "INFO:root:  Epoch 16/150\t Time: 0.080\t Loss: 104.44638824\n",
      "INFO:root:  Epoch 17/150\t Time: 0.074\t Loss: 102.88698578\n",
      "INFO:root:  Epoch 18/150\t Time: 0.073\t Loss: 101.38954926\n",
      "INFO:root:  Epoch 19/150\t Time: 0.076\t Loss: 99.93962860\n",
      "INFO:root:  Epoch 20/150\t Time: 0.073\t Loss: 98.53111267\n",
      "INFO:root:  Epoch 21/150\t Time: 0.073\t Loss: 97.15759277\n",
      "INFO:root:  Epoch 22/150\t Time: 0.071\t Loss: 95.81442261\n",
      "INFO:root:  Epoch 23/150\t Time: 0.071\t Loss: 94.49794769\n",
      "INFO:root:  Epoch 24/150\t Time: 0.082\t Loss: 93.20799255\n",
      "INFO:root:  Epoch 25/150\t Time: 0.082\t Loss: 91.94731903\n",
      "INFO:root:  Epoch 26/150\t Time: 0.077\t Loss: 90.71266937\n",
      "INFO:root:  Epoch 27/150\t Time: 0.083\t Loss: 89.50115967\n",
      "INFO:root:  Epoch 28/150\t Time: 0.106\t Loss: 88.31700897\n",
      "INFO:root:  Epoch 29/150\t Time: 0.088\t Loss: 87.16587830\n",
      "INFO:root:  Epoch 30/150\t Time: 0.079\t Loss: 86.04556274\n",
      "INFO:root:  Epoch 31/150\t Time: 0.078\t Loss: 84.95653534\n",
      "INFO:root:  Epoch 32/150\t Time: 0.074\t Loss: 83.90072632\n",
      "INFO:root:  Epoch 33/150\t Time: 0.073\t Loss: 82.87271118\n",
      "INFO:root:  Epoch 34/150\t Time: 0.076\t Loss: 81.87135315\n",
      "INFO:root:  Epoch 35/150\t Time: 0.074\t Loss: 80.88607025\n",
      "INFO:root:  Epoch 36/150\t Time: 0.072\t Loss: 79.91797638\n",
      "INFO:root:  Epoch 37/150\t Time: 0.071\t Loss: 78.96344757\n",
      "INFO:root:  Epoch 38/150\t Time: 0.070\t Loss: 78.01979065\n",
      "INFO:root:  Epoch 39/150\t Time: 0.074\t Loss: 77.08522034\n",
      "INFO:root:  Epoch 40/150\t Time: 0.074\t Loss: 76.16069794\n",
      "INFO:root:  Epoch 41/150\t Time: 0.075\t Loss: 75.24624634\n",
      "INFO:root:  Epoch 42/150\t Time: 0.076\t Loss: 74.34224701\n",
      "INFO:root:  Epoch 43/150\t Time: 0.074\t Loss: 73.45044708\n",
      "INFO:root:  Epoch 44/150\t Time: 0.073\t Loss: 72.56912231\n",
      "INFO:root:  Epoch 45/150\t Time: 0.078\t Loss: 71.70429230\n",
      "INFO:root:  Epoch 46/150\t Time: 0.077\t Loss: 70.85451508\n",
      "INFO:root:  Epoch 47/150\t Time: 0.076\t Loss: 70.01897430\n",
      "INFO:root:  Epoch 48/150\t Time: 0.075\t Loss: 69.19598389\n",
      "INFO:root:  Epoch 49/150\t Time: 0.078\t Loss: 68.38490295\n",
      "INFO:root:  Epoch 50/150\t Time: 0.078\t Loss: 67.58522034\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:396: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:  Epoch 51/150\t Time: 0.078\t Loss: 67.50574493\n",
      "INFO:root:  Epoch 52/150\t Time: 0.079\t Loss: 67.42637634\n",
      "INFO:root:  Epoch 53/150\t Time: 0.076\t Loss: 67.34716797\n",
      "INFO:root:  Epoch 54/150\t Time: 0.071\t Loss: 67.26802826\n",
      "INFO:root:  Epoch 55/150\t Time: 0.071\t Loss: 67.18891907\n",
      "INFO:root:  Epoch 56/150\t Time: 0.070\t Loss: 67.10990143\n",
      "INFO:root:  Epoch 57/150\t Time: 0.070\t Loss: 67.03090668\n",
      "INFO:root:  Epoch 58/150\t Time: 0.072\t Loss: 66.95195007\n",
      "INFO:root:  Epoch 59/150\t Time: 0.079\t Loss: 66.87305450\n",
      "INFO:root:  Epoch 60/150\t Time: 0.078\t Loss: 66.79418182\n",
      "INFO:root:  Epoch 61/150\t Time: 0.074\t Loss: 66.71535492\n",
      "INFO:root:  Epoch 62/150\t Time: 0.076\t Loss: 66.63653564\n",
      "INFO:root:  Epoch 63/150\t Time: 0.075\t Loss: 66.55776215\n",
      "INFO:root:  Epoch 64/150\t Time: 0.071\t Loss: 66.47907257\n",
      "INFO:root:  Epoch 65/150\t Time: 0.077\t Loss: 66.40045166\n",
      "INFO:root:  Epoch 66/150\t Time: 0.075\t Loss: 66.32189178\n",
      "INFO:root:  Epoch 67/150\t Time: 0.071\t Loss: 66.24333954\n",
      "INFO:root:  Epoch 68/150\t Time: 0.069\t Loss: 66.16481781\n",
      "INFO:root:  Epoch 69/150\t Time: 0.070\t Loss: 66.08636475\n",
      "INFO:root:  Epoch 70/150\t Time: 0.070\t Loss: 66.00801849\n",
      "INFO:root:  Epoch 71/150\t Time: 0.071\t Loss: 65.92993927\n",
      "INFO:root:  Epoch 72/150\t Time: 0.072\t Loss: 65.85195160\n",
      "INFO:root:  Epoch 73/150\t Time: 0.071\t Loss: 65.77399445\n",
      "INFO:root:  Epoch 74/150\t Time: 0.070\t Loss: 65.69606781\n",
      "INFO:root:  Epoch 75/150\t Time: 0.070\t Loss: 65.61815643\n",
      "INFO:root:  Epoch 76/150\t Time: 0.070\t Loss: 65.54024506\n",
      "INFO:root:  Epoch 77/150\t Time: 0.072\t Loss: 65.46235657\n",
      "INFO:root:  Epoch 78/150\t Time: 0.070\t Loss: 65.38456726\n",
      "INFO:root:  Epoch 79/150\t Time: 0.071\t Loss: 65.30683899\n",
      "INFO:root:  Epoch 80/150\t Time: 0.073\t Loss: 65.22916412\n",
      "INFO:root:  Epoch 81/150\t Time: 0.071\t Loss: 65.15155029\n",
      "INFO:root:  Epoch 82/150\t Time: 0.069\t Loss: 65.07407379\n",
      "INFO:root:  Epoch 83/150\t Time: 0.069\t Loss: 64.99669647\n",
      "INFO:root:  Epoch 84/150\t Time: 0.070\t Loss: 64.91941071\n",
      "INFO:root:  Epoch 85/150\t Time: 0.079\t Loss: 64.84222412\n",
      "INFO:root:  Epoch 86/150\t Time: 0.070\t Loss: 64.76511383\n",
      "INFO:root:  Epoch 87/150\t Time: 0.070\t Loss: 64.68806458\n",
      "INFO:root:  Epoch 88/150\t Time: 0.071\t Loss: 64.61103821\n",
      "INFO:root:  Epoch 89/150\t Time: 0.070\t Loss: 64.53405762\n",
      "INFO:root:  Epoch 90/150\t Time: 0.075\t Loss: 64.45712280\n",
      "INFO:root:  Epoch 91/150\t Time: 0.075\t Loss: 64.38026428\n",
      "INFO:root:  Epoch 92/150\t Time: 0.072\t Loss: 64.30344391\n",
      "INFO:root:  Epoch 93/150\t Time: 0.073\t Loss: 64.22671509\n",
      "INFO:root:  Epoch 94/150\t Time: 0.071\t Loss: 64.15012360\n",
      "INFO:root:  Epoch 95/150\t Time: 0.072\t Loss: 64.07352448\n",
      "INFO:root:  Epoch 96/150\t Time: 0.073\t Loss: 63.99693680\n",
      "INFO:root:  Epoch 97/150\t Time: 0.071\t Loss: 63.92042160\n",
      "INFO:root:  Epoch 98/150\t Time: 0.072\t Loss: 63.84390259\n",
      "INFO:root:  Epoch 99/150\t Time: 0.075\t Loss: 63.76743698\n",
      "INFO:root:  Epoch 100/150\t Time: 0.072\t Loss: 63.69104767\n",
      "INFO:root:  Epoch 101/150\t Time: 0.073\t Loss: 63.61476517\n",
      "INFO:root:  Epoch 102/150\t Time: 0.071\t Loss: 63.53852463\n",
      "INFO:root:  Epoch 103/150\t Time: 0.070\t Loss: 63.46235275\n",
      "INFO:root:  Epoch 104/150\t Time: 0.070\t Loss: 63.38634491\n",
      "INFO:root:  Epoch 105/150\t Time: 0.071\t Loss: 63.31040573\n",
      "INFO:root:  Epoch 106/150\t Time: 0.071\t Loss: 63.23453522\n",
      "INFO:root:  Epoch 107/150\t Time: 0.074\t Loss: 63.15876389\n",
      "INFO:root:  Epoch 108/150\t Time: 0.070\t Loss: 63.08302689\n",
      "INFO:root:  Epoch 109/150\t Time: 0.071\t Loss: 63.00742722\n",
      "INFO:root:  Epoch 110/150\t Time: 0.070\t Loss: 62.93190384\n",
      "INFO:root:  Epoch 111/150\t Time: 0.071\t Loss: 62.85643387\n",
      "INFO:root:  Epoch 112/150\t Time: 0.070\t Loss: 62.78099823\n",
      "INFO:root:  Epoch 113/150\t Time: 0.078\t Loss: 62.70562363\n",
      "INFO:root:  Epoch 114/150\t Time: 0.076\t Loss: 62.63031387\n",
      "INFO:root:  Epoch 115/150\t Time: 0.072\t Loss: 62.55506134\n",
      "INFO:root:  Epoch 116/150\t Time: 0.071\t Loss: 62.47983170\n",
      "INFO:root:  Epoch 117/150\t Time: 0.074\t Loss: 62.40464401\n",
      "INFO:root:  Epoch 118/150\t Time: 0.074\t Loss: 62.32951736\n",
      "INFO:root:  Epoch 119/150\t Time: 0.071\t Loss: 62.25443268\n",
      "INFO:root:  Epoch 120/150\t Time: 0.071\t Loss: 62.17942429\n",
      "INFO:root:  Epoch 121/150\t Time: 0.080\t Loss: 62.10445404\n",
      "INFO:root:  Epoch 122/150\t Time: 0.072\t Loss: 62.02959061\n",
      "INFO:root:  Epoch 123/150\t Time: 0.074\t Loss: 61.95481491\n",
      "INFO:root:  Epoch 124/150\t Time: 0.070\t Loss: 61.88011932\n",
      "INFO:root:  Epoch 125/150\t Time: 0.070\t Loss: 61.80552292\n",
      "INFO:root:  Epoch 126/150\t Time: 0.070\t Loss: 61.73093033\n",
      "INFO:root:  Epoch 127/150\t Time: 0.075\t Loss: 61.65644836\n",
      "INFO:root:  Epoch 128/150\t Time: 0.072\t Loss: 61.58205032\n",
      "INFO:root:  Epoch 129/150\t Time: 0.076\t Loss: 61.50773621\n",
      "INFO:root:  Epoch 130/150\t Time: 0.071\t Loss: 61.43349838\n",
      "INFO:root:  Epoch 131/150\t Time: 0.072\t Loss: 61.35932922\n",
      "INFO:root:  Epoch 132/150\t Time: 0.072\t Loss: 61.28524017\n",
      "INFO:root:  Epoch 133/150\t Time: 0.072\t Loss: 61.21123123\n",
      "INFO:root:  Epoch 134/150\t Time: 0.073\t Loss: 61.13729858\n",
      "INFO:root:  Epoch 135/150\t Time: 0.073\t Loss: 61.06341171\n",
      "INFO:root:  Epoch 136/150\t Time: 0.072\t Loss: 60.98961258\n",
      "INFO:root:  Epoch 137/150\t Time: 0.073\t Loss: 60.91588211\n",
      "INFO:root:  Epoch 138/150\t Time: 0.072\t Loss: 60.84223938\n",
      "INFO:root:  Epoch 139/150\t Time: 0.071\t Loss: 60.76868439\n",
      "INFO:root:  Epoch 140/150\t Time: 0.074\t Loss: 60.69522858\n",
      "INFO:root:  Epoch 141/150\t Time: 0.074\t Loss: 60.62184525\n",
      "INFO:root:  Epoch 142/150\t Time: 0.071\t Loss: 60.54854584\n",
      "INFO:root:  Epoch 143/150\t Time: 0.072\t Loss: 60.47534943\n",
      "INFO:root:  Epoch 144/150\t Time: 0.071\t Loss: 60.40216446\n",
      "INFO:root:  Epoch 145/150\t Time: 0.073\t Loss: 60.32908249\n",
      "INFO:root:  Epoch 146/150\t Time: 0.075\t Loss: 60.25608444\n",
      "INFO:root:  Epoch 147/150\t Time: 0.072\t Loss: 60.18315125\n",
      "INFO:root:  Epoch 148/150\t Time: 0.077\t Loss: 60.11032104\n",
      "INFO:root:  Epoch 149/150\t Time: 0.072\t Loss: 60.03758621\n",
      "INFO:root:  Epoch 150/150\t Time: 0.073\t Loss: 59.96492004\n",
      "INFO:root:Pretraining time: 11.278\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test set Loss: 59.80253220\n",
      "INFO:root:Test set AUC: 100.00%\n",
      "INFO:root:Autoencoder testing time: 0.008\n",
      "INFO:root:Finished testing autoencoder.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info('Pretraining: %s' % pretrain)\n",
    "if pretrain:\n",
    "    # Log pretraining details\n",
    "    logger.info('Pretraining optimizer: %s' % cfg.settings['ae_optimizer_name'])\n",
    "    logger.info('Pretraining learning rate: %g' % cfg.settings['ae_lr'])\n",
    "    logger.info('Pretraining epochs: %d' % cfg.settings['ae_n_epochs'])\n",
    "    logger.info('Pretraining learning rate scheduler milestones: %s' % (cfg.settings['ae_lr_milestone'],))\n",
    "    logger.info('Pretraining batch size: %d' % cfg.settings['ae_batch_size'])\n",
    "    logger.info('Pretraining weight decay: %g' % cfg.settings['ae_weight_decay'])\n",
    "\n",
    "    # Pretrain model on dataset (via autoencoder)\n",
    "    deep_SVDD.pretrain(dataset,\n",
    "                        optimizer_name=cfg.settings['ae_optimizer_name'],\n",
    "                        lr=cfg.settings['ae_lr'],\n",
    "                        n_epochs=cfg.settings['ae_n_epochs'],\n",
    "                        lr_milestones=cfg.settings['ae_lr_milestone'],\n",
    "                        batch_size=cfg.settings['ae_batch_size'],\n",
    "                        weight_decay=cfg.settings['ae_weight_decay'],\n",
    "                        device=device,\n",
    "                        n_jobs_dataloader=n_jobs_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 150\n",
      "INFO:root:Training learning rate scheduler milestones: [50]\n",
      "INFO:root:Training batch size: 200\n",
      "INFO:root:Training weight decay: 5e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Log training details\n",
    "logger.info('Training optimizer: %s' % cfg.settings['optimizer_name'])\n",
    "logger.info('Training learning rate: %g' % cfg.settings['lr'])\n",
    "logger.info('Training epochs: %d' % cfg.settings['n_epochs'])\n",
    "logger.info('Training learning rate scheduler milestones: %s' % (cfg.settings['lr_milestone'],))\n",
    "logger.info('Training batch size: %d' % cfg.settings['batch_size'])\n",
    "logger.info('Training weight decay: %g' % cfg.settings['weight_decay'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "INFO:root:  Epoch 1/150\t Time: 0.046\t Loss: 2.91983056\n",
      "INFO:root:  Epoch 2/150\t Time: 0.040\t Loss: 2.83327031\n",
      "INFO:root:  Epoch 3/150\t Time: 0.036\t Loss: 2.75456834\n",
      "INFO:root:  Epoch 4/150\t Time: 0.038\t Loss: 2.68134212\n",
      "INFO:root:  Epoch 5/150\t Time: 0.037\t Loss: 2.61239862\n",
      "INFO:root:  Epoch 6/150\t Time: 0.036\t Loss: 2.54675746\n",
      "INFO:root:  Epoch 7/150\t Time: 0.039\t Loss: 2.48363495\n",
      "INFO:root:  Epoch 8/150\t Time: 0.037\t Loss: 2.42251062\n",
      "INFO:root:  Epoch 9/150\t Time: 0.038\t Loss: 2.36317086\n",
      "INFO:root:  Epoch 10/150\t Time: 0.034\t Loss: 2.30530500\n",
      "INFO:root:  Epoch 11/150\t Time: 0.035\t Loss: 2.24903297\n",
      "INFO:root:  Epoch 12/150\t Time: 0.036\t Loss: 2.19430470\n",
      "INFO:root:  Epoch 13/150\t Time: 0.037\t Loss: 2.14095807\n",
      "INFO:root:  Epoch 14/150\t Time: 0.036\t Loss: 2.08902669\n",
      "INFO:root:  Epoch 15/150\t Time: 0.039\t Loss: 2.03858662\n",
      "INFO:root:  Epoch 16/150\t Time: 0.035\t Loss: 1.98969066\n",
      "INFO:root:  Epoch 17/150\t Time: 0.036\t Loss: 1.94226432\n",
      "INFO:root:  Epoch 18/150\t Time: 0.035\t Loss: 1.89640987\n",
      "INFO:root:  Epoch 19/150\t Time: 0.034\t Loss: 1.85202086\n",
      "INFO:root:  Epoch 20/150\t Time: 0.034\t Loss: 1.80911052\n",
      "INFO:root:  Epoch 21/150\t Time: 0.036\t Loss: 1.76757193\n",
      "INFO:root:  Epoch 22/150\t Time: 0.035\t Loss: 1.72724688\n",
      "INFO:root:  Epoch 23/150\t Time: 0.034\t Loss: 1.68816555\n",
      "INFO:root:  Epoch 24/150\t Time: 0.034\t Loss: 1.65033877\n",
      "INFO:root:  Epoch 25/150\t Time: 0.034\t Loss: 1.61371839\n",
      "INFO:root:  Epoch 26/150\t Time: 0.036\t Loss: 1.57822156\n",
      "INFO:root:  Epoch 27/150\t Time: 0.039\t Loss: 1.54389405\n",
      "INFO:root:  Epoch 28/150\t Time: 0.038\t Loss: 1.51065326\n",
      "INFO:root:  Epoch 29/150\t Time: 0.036\t Loss: 1.47845066\n",
      "INFO:root:  Epoch 30/150\t Time: 0.035\t Loss: 1.44730806\n",
      "INFO:root:  Epoch 31/150\t Time: 0.037\t Loss: 1.41710091\n",
      "INFO:root:  Epoch 32/150\t Time: 0.036\t Loss: 1.38786995\n",
      "INFO:root:  Epoch 33/150\t Time: 0.034\t Loss: 1.35953212\n",
      "INFO:root:  Epoch 34/150\t Time: 0.035\t Loss: 1.33208668\n",
      "INFO:root:  Epoch 35/150\t Time: 0.035\t Loss: 1.30533171\n",
      "INFO:root:  Epoch 36/150\t Time: 0.043\t Loss: 1.27931893\n",
      "INFO:root:  Epoch 37/150\t Time: 0.037\t Loss: 1.25404060\n",
      "INFO:root:  Epoch 38/150\t Time: 0.037\t Loss: 1.22945690\n",
      "INFO:root:  Epoch 39/150\t Time: 0.035\t Loss: 1.20559025\n",
      "INFO:root:  Epoch 40/150\t Time: 0.038\t Loss: 1.18242443\n",
      "INFO:root:  Epoch 41/150\t Time: 0.036\t Loss: 1.15990674\n",
      "INFO:root:  Epoch 42/150\t Time: 0.042\t Loss: 1.13803840\n",
      "INFO:root:  Epoch 43/150\t Time: 0.039\t Loss: 1.11676931\n",
      "INFO:root:  Epoch 44/150\t Time: 0.038\t Loss: 1.09612226\n",
      "INFO:root:  Epoch 45/150\t Time: 0.039\t Loss: 1.07607877\n",
      "INFO:root:  Epoch 46/150\t Time: 0.037\t Loss: 1.05663264\n",
      "INFO:root:  Epoch 47/150\t Time: 0.036\t Loss: 1.03773558\n",
      "INFO:root:  Epoch 48/150\t Time: 0.036\t Loss: 1.01936591\n",
      "INFO:root:  Epoch 49/150\t Time: 0.036\t Loss: 1.00154722\n",
      "INFO:root:  Epoch 50/150\t Time: 0.035\t Loss: 0.98423707\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:396: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:  Epoch 51/150\t Time: 0.047\t Loss: 0.98254013\n",
      "INFO:root:  Epoch 52/150\t Time: 0.043\t Loss: 0.98085916\n",
      "INFO:root:  Epoch 53/150\t Time: 0.040\t Loss: 0.97919345\n",
      "INFO:root:  Epoch 54/150\t Time: 0.038\t Loss: 0.97754109\n",
      "INFO:root:  Epoch 55/150\t Time: 0.036\t Loss: 0.97590107\n",
      "INFO:root:  Epoch 56/150\t Time: 0.034\t Loss: 0.97426945\n",
      "INFO:root:  Epoch 57/150\t Time: 0.039\t Loss: 0.97264719\n",
      "INFO:root:  Epoch 58/150\t Time: 0.036\t Loss: 0.97103328\n",
      "INFO:root:  Epoch 59/150\t Time: 0.037\t Loss: 0.96942621\n",
      "INFO:root:  Epoch 60/150\t Time: 0.036\t Loss: 0.96782380\n",
      "INFO:root:  Epoch 61/150\t Time: 0.038\t Loss: 0.96622717\n",
      "INFO:root:  Epoch 62/150\t Time: 0.036\t Loss: 0.96463603\n",
      "INFO:root:  Epoch 63/150\t Time: 0.036\t Loss: 0.96304882\n",
      "INFO:root:  Epoch 64/150\t Time: 0.035\t Loss: 0.96146506\n",
      "INFO:root:  Epoch 65/150\t Time: 0.036\t Loss: 0.95988613\n",
      "INFO:root:  Epoch 66/150\t Time: 0.036\t Loss: 0.95831209\n",
      "INFO:root:  Epoch 67/150\t Time: 0.035\t Loss: 0.95674002\n",
      "INFO:root:  Epoch 68/150\t Time: 0.037\t Loss: 0.95516938\n",
      "INFO:root:  Epoch 69/150\t Time: 0.035\t Loss: 0.95360261\n",
      "INFO:root:  Epoch 70/150\t Time: 0.040\t Loss: 0.95203930\n",
      "INFO:root:  Epoch 71/150\t Time: 0.037\t Loss: 0.95047772\n",
      "INFO:root:  Epoch 72/150\t Time: 0.036\t Loss: 0.94891798\n",
      "INFO:root:  Epoch 73/150\t Time: 0.036\t Loss: 0.94735986\n",
      "INFO:root:  Epoch 74/150\t Time: 0.035\t Loss: 0.94580430\n",
      "INFO:root:  Epoch 75/150\t Time: 0.036\t Loss: 0.94425088\n",
      "INFO:root:  Epoch 76/150\t Time: 0.036\t Loss: 0.94270164\n",
      "INFO:root:  Epoch 77/150\t Time: 0.036\t Loss: 0.94115430\n",
      "INFO:root:  Epoch 78/150\t Time: 0.037\t Loss: 0.93960834\n",
      "INFO:root:  Epoch 79/150\t Time: 0.034\t Loss: 0.93806219\n",
      "INFO:root:  Epoch 80/150\t Time: 0.036\t Loss: 0.93651712\n",
      "INFO:root:  Epoch 81/150\t Time: 0.035\t Loss: 0.93497378\n",
      "INFO:root:  Epoch 82/150\t Time: 0.036\t Loss: 0.93343174\n",
      "INFO:root:  Epoch 83/150\t Time: 0.036\t Loss: 0.93189049\n",
      "INFO:root:  Epoch 84/150\t Time: 0.035\t Loss: 0.93035054\n",
      "INFO:root:  Epoch 85/150\t Time: 0.036\t Loss: 0.92881125\n",
      "INFO:root:  Epoch 86/150\t Time: 0.034\t Loss: 0.92727268\n",
      "INFO:root:  Epoch 87/150\t Time: 0.035\t Loss: 0.92573488\n",
      "INFO:root:  Epoch 88/150\t Time: 0.037\t Loss: 0.92419797\n",
      "INFO:root:  Epoch 89/150\t Time: 0.037\t Loss: 0.92266285\n",
      "INFO:root:  Epoch 90/150\t Time: 0.036\t Loss: 0.92112911\n",
      "INFO:root:  Epoch 91/150\t Time: 0.038\t Loss: 0.91959745\n",
      "INFO:root:  Epoch 92/150\t Time: 0.040\t Loss: 0.91806751\n",
      "INFO:root:  Epoch 93/150\t Time: 0.037\t Loss: 0.91653883\n",
      "INFO:root:  Epoch 94/150\t Time: 0.041\t Loss: 0.91501188\n",
      "INFO:root:  Epoch 95/150\t Time: 0.036\t Loss: 0.91348606\n",
      "INFO:root:  Epoch 96/150\t Time: 0.038\t Loss: 0.91196173\n",
      "INFO:root:  Epoch 97/150\t Time: 0.037\t Loss: 0.91043997\n",
      "INFO:root:  Epoch 98/150\t Time: 0.036\t Loss: 0.90892059\n",
      "INFO:root:  Epoch 99/150\t Time: 0.035\t Loss: 0.90740234\n",
      "INFO:root:  Epoch 100/150\t Time: 0.038\t Loss: 0.90588480\n",
      "INFO:root:  Epoch 101/150\t Time: 0.036\t Loss: 0.90436816\n",
      "INFO:root:  Epoch 102/150\t Time: 0.041\t Loss: 0.90285093\n",
      "INFO:root:  Epoch 103/150\t Time: 0.037\t Loss: 0.90133458\n",
      "INFO:root:  Epoch 104/150\t Time: 0.036\t Loss: 0.89982033\n",
      "INFO:root:  Epoch 105/150\t Time: 0.037\t Loss: 0.89830738\n",
      "INFO:root:  Epoch 106/150\t Time: 0.036\t Loss: 0.89679718\n",
      "INFO:root:  Epoch 107/150\t Time: 0.036\t Loss: 0.89528978\n",
      "INFO:root:  Epoch 108/150\t Time: 0.037\t Loss: 0.89378417\n",
      "INFO:root:  Epoch 109/150\t Time: 0.040\t Loss: 0.89228100\n",
      "INFO:root:  Epoch 110/150\t Time: 0.040\t Loss: 0.89077908\n",
      "INFO:root:  Epoch 111/150\t Time: 0.040\t Loss: 0.88927764\n",
      "INFO:root:  Epoch 112/150\t Time: 0.038\t Loss: 0.88777775\n",
      "INFO:root:  Epoch 113/150\t Time: 0.038\t Loss: 0.88627994\n",
      "INFO:root:  Epoch 114/150\t Time: 0.040\t Loss: 0.88478392\n",
      "INFO:root:  Epoch 115/150\t Time: 0.038\t Loss: 0.88328981\n",
      "INFO:root:  Epoch 116/150\t Time: 0.037\t Loss: 0.88179809\n",
      "INFO:root:  Epoch 117/150\t Time: 0.038\t Loss: 0.88030767\n",
      "INFO:root:  Epoch 118/150\t Time: 0.039\t Loss: 0.87882000\n",
      "INFO:root:  Epoch 119/150\t Time: 0.038\t Loss: 0.87733507\n",
      "INFO:root:  Epoch 120/150\t Time: 0.039\t Loss: 0.87585247\n",
      "INFO:root:  Epoch 121/150\t Time: 0.040\t Loss: 0.87437016\n",
      "INFO:root:  Epoch 122/150\t Time: 0.041\t Loss: 0.87288934\n",
      "INFO:root:  Epoch 123/150\t Time: 0.038\t Loss: 0.87141067\n",
      "INFO:root:  Epoch 124/150\t Time: 0.041\t Loss: 0.86993450\n",
      "INFO:root:  Epoch 125/150\t Time: 0.039\t Loss: 0.86846232\n",
      "INFO:root:  Epoch 126/150\t Time: 0.039\t Loss: 0.86699313\n",
      "INFO:root:  Epoch 127/150\t Time: 0.038\t Loss: 0.86552721\n",
      "INFO:root:  Epoch 128/150\t Time: 0.041\t Loss: 0.86406326\n",
      "INFO:root:  Epoch 129/150\t Time: 0.040\t Loss: 0.86260080\n",
      "INFO:root:  Epoch 130/150\t Time: 0.037\t Loss: 0.86114120\n",
      "INFO:root:  Epoch 131/150\t Time: 0.037\t Loss: 0.85968190\n",
      "INFO:root:  Epoch 132/150\t Time: 0.038\t Loss: 0.85822475\n",
      "INFO:root:  Epoch 133/150\t Time: 0.039\t Loss: 0.85676754\n",
      "INFO:root:  Epoch 134/150\t Time: 0.039\t Loss: 0.85531187\n",
      "INFO:root:  Epoch 135/150\t Time: 0.039\t Loss: 0.85385734\n",
      "INFO:root:  Epoch 136/150\t Time: 0.042\t Loss: 0.85240465\n",
      "INFO:root:  Epoch 137/150\t Time: 0.039\t Loss: 0.85095453\n",
      "INFO:root:  Epoch 138/150\t Time: 0.038\t Loss: 0.84950566\n",
      "INFO:root:  Epoch 139/150\t Time: 0.040\t Loss: 0.84805900\n",
      "INFO:root:  Epoch 140/150\t Time: 0.043\t Loss: 0.84661531\n",
      "INFO:root:  Epoch 141/150\t Time: 0.039\t Loss: 0.84517550\n",
      "INFO:root:  Epoch 142/150\t Time: 0.037\t Loss: 0.84373784\n",
      "INFO:root:  Epoch 143/150\t Time: 0.038\t Loss: 0.84230161\n",
      "INFO:root:  Epoch 144/150\t Time: 0.040\t Loss: 0.84086555\n",
      "INFO:root:  Epoch 145/150\t Time: 0.038\t Loss: 0.83943087\n",
      "INFO:root:  Epoch 146/150\t Time: 0.040\t Loss: 0.83799797\n",
      "INFO:root:  Epoch 147/150\t Time: 0.042\t Loss: 0.83656681\n",
      "INFO:root:  Epoch 148/150\t Time: 0.036\t Loss: 0.83513743\n",
      "INFO:root:  Epoch 149/150\t Time: 0.039\t Loss: 0.83370978\n",
      "INFO:root:  Epoch 150/150\t Time: 0.039\t Loss: 0.83228439\n",
      "INFO:root:Training time: 5.748\n",
      "INFO:root:Finished training.\n"
     ]
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "deep_SVDD.train(dataset,\n",
    "                optimizer_name=cfg.settings['optimizer_name'],\n",
    "                lr=cfg.settings['lr'],\n",
    "                n_epochs=cfg.settings['n_epochs'],\n",
    "                lr_milestones=cfg.settings['lr_milestone'],\n",
    "                batch_size=cfg.settings['batch_size'],\n",
    "                weight_decay=cfg.settings['weight_decay'],\n",
    "                device=device,\n",
    "                n_jobs_dataloader=n_jobs_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting testing...\n",
      "INFO:root:Testing time: 0.004\n",
      "INFO:root:Test set AUC: 100.00%\n",
      "INFO:root:Finished testing.\n"
     ]
    }
   ],
   "source": [
    "deep_SVDD.test(dataset, device=device, n_jobs_dataloader=n_jobs_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot most anomalous and most normal (within-class) test samples\n",
    "indices, labels, scores = zip(*deep_SVDD.results['test_scores'])\n",
    "indices, labels, scores = np.array(indices), np.array(labels), np.array(scores)\n",
    "idx_sorted = indices[labels == 0][np.argsort(scores[labels == 0])]  # sorted from lowest to highest anomaly score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83345801, 1.32908773, 0.22859728, 0.20104563, 0.22859728,\n",
       "       1.12149811])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 5, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring index sorting by score value (low -> high)\n",
    "score_idx = np.argsort(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name in ('mnist', 'cifar10', 'custom'):\n",
    "\n",
    "    if dataset_name == 'mnist':\n",
    "        X_normals = dataset.test_set.test_data[idx_sorted[:32], ...].unsqueeze(1)\n",
    "        X_outliers = dataset.test_set.test_data[idx_sorted[-32:], ...].unsqueeze(1)\n",
    "        plot_images_grid(X_normals, export_img=xp_path + '/normals', title='Most normal examples', padding=2)\n",
    "        plot_images_grid(X_outliers, export_img=xp_path + '/outliers', title='Most anomalous examples', padding=2)\n",
    "\n",
    "    if dataset_name == 'cifar10':\n",
    "        X_normals = torch.tensor(np.transpose(dataset.test_set.test_data[idx_sorted[:32], ...], (0, 3, 1, 2)))\n",
    "        X_outliers = torch.tensor(np.transpose(dataset.test_set.test_data[idx_sorted[-32:], ...], (0, 3, 1, 2)))\n",
    "        plot_images_grid(X_normals, export_img=xp_path + '/normals', title='Most normal examples', padding=2)\n",
    "        plot_images_grid(X_outliers, export_img=xp_path + '/outliers', title='Most anomalous examples', padding=2)\n",
    "\n",
    "    if dataset_name == 'custom':\n",
    "#         X_normals = dataset.test_set.test_data[0][idx_sorted[:32], ...].unsqueeze(1)\n",
    "#         X_outliers = dataset.test_set.test_data[0][idx_sorted[-32:], ...].unsqueeze(1)\n",
    "        X_normals = torch.tensor(dataset.test_set.test_data)[idx_sorted[:32], ...][0]\n",
    "        X_outliers = torch.tensor(dataset.test_set.test_data)[idx_sorted[-32:], ...][0]\n",
    "        \n",
    "        normal_idx = score_idx[-math.ceil(len(score_idx)/2):]\n",
    "        outliers_idx = score_idx[:math.ceil(len(score_idx)/2)]\n",
    "        print('Normal:')\n",
    "        show_image(normal_idx, dataset.test_set.test_data, labels, export_img=xp_path + '/normals')\n",
    "        print('Abnormal:')\n",
    "        show_image(outliers_idx, dataset.test_set.test_data, labels, export_img=xp_path + '/outliers')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.test_set.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5]),\n",
       " array([1, 1, 0, 0, 0, 1]),\n",
       " array([0.83345801, 1.32908773, 0.22859728, 0.20104563, 0.22859728,\n",
       "        1.12149811]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices, labels, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type module is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-905a2629c498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdeep_SVDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/results.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdeep_SVDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/model.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/ml-occ-deepsvdd/src/utils/config.py\u001b[0m in \u001b[0;36msave_config\u001b[0;34m(self, export_json)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type module is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# Save results, model, and configuration\n",
    "deep_SVDD.save_results(export_json=xp_path + '/results.json')\n",
    "deep_SVDD.save_model(export_model=xp_path + '/model.tar')\n",
    "cfg.save_config(export_json=xp_path + '/config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model and print to file\n",
    "if apply_model:\n",
    "    deep_SVDD.apply_model(export_file=xp_path + '/apply_output.txt', dataset=dataset, device=device, n_jobs_dataloader=n_jobs_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}